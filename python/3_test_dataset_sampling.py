#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
vLLM-based multi-question generation + PRM/ORM í›„ì²˜ë¦¬
 - ì„ íƒí˜• ë°ì´í„°(ì˜ˆ: med_qa, medmc_qa â€¦)ì™€
   ì£¼ê´€ì‹ ë°ì´í„°(ì˜ˆ: nejm, osce) ë¥¼ í•œ íŒŒì´í”„ë¼ì¸ì—ì„œ ì²˜ë¦¬
 - ìµœì¢… JSON êµ¬ì¡° ì˜ˆì‹œëŠ” README í•˜ë‹¨ ì°¸ì¡°
"""
import argparse
import os
import json
import re
import math
import string
import torch
from collections import defaultdict

from huggingface_hub import login
from vllm import LLM, SamplingParams

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ìƒìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MULTIPLE_CHOICE_SOURCES = {"med_qa", "medmc_qa", "ddxplus", "mmlu_anatomy", 
                          "mmlu_clinical_knowledge", "mmlu_college_biology", 
                          "mmlu_college_medicine", "mmlu_medical_genetics", 
                          "mmlu_professional_medicine", "pubmed_qa"}            
OPEN_SOURCES           = {"nejm", "osce"}                   # ì£¼ê´€ì‹

SYSTEM_PROMPT = (
    "Solve the following question step-by-step. "
    "Do not analyze individual options in a single step. "
    "Each step of your explanation must start with 'Step {number}:' format. "
    "You must provide the answer using the phrase 'the answer is (option alphabet)' at the end of your step."
)

OPEN_SYSTEM_PROMPT = (
"Solve the following question step-by-step. Each step of your explanation must start with \'## Step {number}: \' format. The final answer must output a concise and clearly defined diagnostic term.  You must provide the final answer using the phrase \'## Final Diagnosis: {Disease name}\' at the end of your final step. Please refer to the following example. ## Final Diagnosis: Multiple Sclerosis"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì¸ì íŒŒì„œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_args():
    p = argparse.ArgumentParser(description="vLLM chatting ê¸°ë°˜ ë¬¸ì œ í’€ì´ ìƒì„±")
    p.add_argument("--hf_token",     type=str, required=True)
    p.add_argument("--model_path",   type=str, required=True)
    p.add_argument("--gpu_id",       type=str, required=True)
    p.add_argument("--input_file",   type=str, required=True)
    p.add_argument("--output_dir",   type=str, required=True)
    p.add_argument("--repeat_count", type=int, required=True)
    p.add_argument("--temperature",  type=float, required=True)
    p.add_argument("--top_k",        type=int, required=True)
    p.add_argument("--max_tokens",   type=int, required=True)
    p.add_argument("--data_source_list", type=str, help="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì²˜ë¦¬í•  ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡")
    return p.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2-A. ì „ì²˜ë¦¬ (ë¬¸ì œ í…ìŠ¤íŠ¸ ì¡°ë¦½)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_question(qdata):
    ds       = qdata["data_source"].lower()
    qid      = qdata["question_id"]
    question = qdata["question"].strip()
    orig_ans = qdata["correct_answer"].strip()              # <<< ë³€ê²½: ì›ë³¸ correct_answer ë³´ì¡´
    opts     = qdata.get("options", [])                     # <<< ë³€ê²½: ì›ë³¸ options ë³´ì¡´
    related_docs = qdata.get("related_docs", [])            # <<< ì¶”ê°€: related_docs í•„ë“œ ê°€ì ¸ì˜¤ê¸°

    if ds in MULTIPLE_CHOICE_SOURCES and opts:
        # ì˜µì…˜ì— (A),(B)... ë¼ë²¨ ë¶™ì´ê¸°
        labels = [f"({c})" for c in string.ascii_uppercase[:len(opts)]]
        opts_text = "\n".join(f"{lab} {opt}" for lab, opt in zip(labels, opts))
        full_q = f"{question}\n\n{opts_text}"
        gt     = orig_ans.upper()                             # <<< ë³€ê²½: MC ì •ë‹µ ì•ŒíŒŒë²³
    else:
        # ì£¼ê´€ì‹ì€ ì§ˆë¬¸ë§Œ
        full_q = question
        gt     = orig_ans                                   # <<< ë³€ê²½: open ì •ë‹µ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ

    return {
        "question_id":          qid,
        "data_source":          ds,                         # <<< ë³€ê²½: data_source ë³´ì¡´
        "question":             full_q,
        "options":              opts,                       # <<< ë³€ê²½: options ë¦¬ìŠ¤íŠ¸ ë³´ì¡´
        "correct_answer":       orig_ans,                   # <<< ë³€ê²½: ì›ë³¸ correct_answer ë³´ì¡´
        "ground_truth_for_eval":gt,                         # <<< ë³€ê²½: í‰ê°€ìš© ì •ë‹µ êµ¬ë¶„ ì €ì¥
        "related_docs":         related_docs                # <<< ì¶”ê°€: related_docs í•„ë“œ ë³´ì¡´
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2-B. LLM ì‘ë‹µ í›„ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STEP_PATTERN = r'(?:## )?Step \d+:'

def extract_steps_from_text(txt: str):
    """## Step n: â€¦ êµ¬ê°„ë³„ ìŠ¬ë¼ì´ìŠ¤"""
    mts = list(re.finditer(STEP_PATTERN, txt))
    if not mts:
        return [txt.strip()] if txt.strip() else []
    steps = []
    for i, m in enumerate(mts):
        start = m.start()
        end   = mts[i+1].start() if i+1 < len(mts) else len(txt)
        steps.append(txt[start:end].strip().replace("## ", ""))
    return steps

# â‘  ì„ íƒí˜• ì •ë‹µ ì¶”ì¶œ
def extract_answer_from_text(txt: str):
    txt = txt.lower()

    # \boxed{b}
    m = re.findall(r'\\boxed\{\(?([a-z])\)?\}', txt)
    if m:
        return m[-1].upper()

    # the answer is b
    m_iter = re.finditer(r'(?:answer is|the answer is|final answer is)\s*:?\s*\(?([a-z])\)?',
                         txt, flags=re.I)
    m_list = list(m_iter)
    if m_list:
        return m_list[-1].group(1).upper()
    return None

# â‘¡ ì£¼ê´€ì‹ ì •ë‹µ ì¶”ì¶œ
def open_extract_answer_from_text(txt: str):
    ptns = [
        r"## Final Diagnosis:\s*(.*?)(?:\n|$)",
        r"Final Diagnosis:\s*(.*?)(?:\n|$)",
        r"diagnosis is\s*(.*?)(?:\.|$)"
    ]
    for p in ptns:
        m = re.search(p, txt, flags=re.I)
        if m:
            return m.group(1).strip()
    return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2-C. PRM/ORM ë³€í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prm_process_solution(txt: str):
    no_nl = txt.replace("\n", " ")
    mts = list(re.finditer(STEP_PATTERN, no_nl))
    if not mts:
        return no_nl.strip() + " ĞºĞ¸" if no_nl.strip() else ""
    head = no_nl[:mts[0].start()].strip()
    steps = []
    for i, m in enumerate(mts):
        start = m.start()
        end   = mts[i+1].start() if i+1 < len(mts) else len(no_nl)
        steps.append(no_nl[start:end].strip().replace("## ", "") + " ĞºĞ¸")
    if head:
        steps.insert(0, head)
    return " ".join(steps)

def orm_process_solution(txt: str):
    return txt.replace("\n", " ") + " ĞºĞ¸"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. JSON ì…ì¶œë ¥ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_json(path):   return json.load(open(path,  encoding="utf-8"))
def save_json(obj, p): json.dump(obj, open(p, "w", encoding="utf-8"),
                                 ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-A. í”„ë¡¬í”„íŠ¸ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_prompts(qdatas, n_samples):
    convs, meta = [], []
    for q in qdatas:
        sys_prompt = OPEN_SYSTEM_PROMPT if q["data_source"] in OPEN_SOURCES else SYSTEM_PROMPT
        conv_tmpl  = [
            {"role": "system", "content": sys_prompt},
            {"role": "user",   "content": q["question"]}
        ]
        for _ in range(n_samples):
            convs.append(conv_tmpl)
            meta.append({
                "question_id":  q["question_id"],
                "ground_truth": q["ground_truth_for_eval"],
                "data_source":  q["data_source"]
            })
    return convs, meta

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-B. vLLM ì‹¤í–‰ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def llm_chat(llm, samp_params, convs, meta):
    outs = llm.chat(convs, samp_params)
    bucket = defaultdict(lambda: {"generated_texts": []})
    for i, o in enumerate(outs):
        qid = meta[i]["question_id"]
        bucket[qid]["generated_texts"].append(o.outputs[0].text)
    return bucket

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_all(qdatas, repeat_cnt, llm, samp_params):
    first_cnt = math.ceil(repeat_cnt * 1.25)

    # 1ì°¨
    convs1, meta1 = collect_prompts(qdatas, first_cnt)
    res1 = llm_chat(llm, samp_params, convs1, meta1)

    valid = defaultdict(list)
    for q in qdatas:
        qid = q["question_id"]
        for txt in res1[qid]["generated_texts"]:
            if 2 < len(extract_steps_from_text(txt)) < 10:
                valid[qid].append(txt)

    # ë¶€ì¡±ë¶„ ê³„ì‚° í›„ 2ì°¨
    need2 = [q for q in qdatas if len(valid[q["question_id"]]) < repeat_cnt]
    if need2:
        convs2, meta2 = [], []
        for q in need2:
            lack = (repeat_cnt - len(valid[q["question_id"]])) * 3
            sys_prompt = OPEN_SYSTEM_PROMPT if q["data_source"] in OPEN_SOURCES else SYSTEM_PROMPT
            tmpl = [
                {"role": "system", "content": sys_prompt},
                {"role": "user",   "content": q["question"]}
            ]
            for _ in range(lack):
                convs2.append(tmpl)
                meta2.append({
                    "question_id":  q["question_id"],
                    "ground_truth": q["ground_truth_for_eval"],
                    "data_source":  q["data_source"]
                })
        res2 = llm_chat(llm, samp_params, convs2, meta2)
        for q in need2:
            qid = q["question_id"]
            for txt in res2[qid]["generated_texts"]:
                if 2 < len(extract_steps_from_text(txt)) < 10:
                    valid[qid].append(txt)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìµœì¢… JSON ì¡°ë¦½ â”€â”€â”€â”€â”€â”€â”€â”€â”€
    outputs = []
    for q in qdatas:
        qid  = q["question_id"]
        ds   = q["data_source"]
        gt   = q["ground_truth_for_eval"]
        sols = []
        for txt in valid[qid][:repeat_cnt]:
            if ds in OPEN_SOURCES:
                pred = open_extract_answer_from_text(txt)
                score = "None"
            else:
                pred  = extract_answer_from_text(txt)
                score = int(pred == gt) if pred else 0
            sols.append({
                "solution": txt,
                "prm_processed_solution": prm_process_solution(txt),
                "orm_processed_solution": orm_process_solution(txt),
                "answer": pred,
                "score": score
            })
        outputs.append({
            "question_id":    qid,
            "data_source":    ds,           # â† ì¶”ê°€
            "question":       q["question"],
            "correct_answer": gt,
            "solutions":      sols,
            "related_docs":   q["related_docs"],  # â† ìˆ˜ì •: related_docs í•„ë“œ ì¶”ê°€
        })
    return outputs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ë©”ì¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    args = parse_args()
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id

    login(args.hf_token)
    print("âœ…  Hugging Face login ì™„ë£Œ")

    llm = LLM(
        model=args.model_path,
        device="cuda",
        dtype="bfloat16",
        max_model_len=args.max_tokens,
        gpu_memory_utilization=0.95,
    )
    samp = SamplingParams(
        temperature=args.temperature,
        top_k=args.top_k,
        max_tokens=args.max_tokens
    )

    raw = load_json(args.input_file)
    print(f"ğŸ“‚ ë¡œë“œ: {len(raw)} ë¬¸ì œ")

    # íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ë§Œ ì²˜ë¦¬
    data_source_tag = "all"
    if args.data_source_list:
        data_sources = [ds.strip() for ds in args.data_source_list.split(',')]
        filtered_raw = [q for q in raw if q["data_source"].lower() in data_sources]
        print(f"ğŸ” í•„í„°ë§: {len(filtered_raw)}/{len(raw)} ë¬¸ì œ (ë°ì´í„° ì†ŒìŠ¤: {', '.join(data_sources)})")
        raw = filtered_raw
        data_source_tag = data_sources[0]  # ì²« ë²ˆì§¸ ë°ì´í„° ì†ŒìŠ¤ë§Œ íŒŒì¼ëª…ì— ì‚¬ìš©

    transformed = [format_question(q) for q in raw]

    dataset = generate_all(transformed, args.repeat_count, llm, samp)

    os.makedirs(args.output_dir, exist_ok=True)
    
    # ëª¨ë¸ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ)
    model_name = os.path.basename(args.model_path.rstrip('/'))
    
    # íŒŒì¼ëª… ë™ì  ìƒì„±
    base = os.path.splitext(os.path.basename(args.input_file))[0]
    out_path = os.path.join(args.output_dir, f"{base}_{model_name}_{data_source_tag}_{args.repeat_count}.json")
    save_json(dataset, out_path)
    print(f"ğŸ‰  Done â†’ {out_path}")

if __name__ == "__main__":
    main()
