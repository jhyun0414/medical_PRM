#!/usr/bin/env python
# coding: utf-8
"""
Run PRM evaluation with optional RAG support.
"""

import argparse
import os
import json
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import login
import accelerate  # ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ì— ìˆë˜ import ìœ ì§€
from collections import Counter
# ----------------------------------------------------------------------
# 1. ì¸ì íŒŒì„œ
# ----------------------------------------------------------------------
def parse_args():
    parser = argparse.ArgumentParser(
        description="Run PRM evaluation (RAG on/off selectable)."
    )

    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument("--model_save_path", type=str, required=True,
                        help="Path to the saved model directory")
    parser.add_argument("--device", type=str, default="",
                        help="CUDA visible devices (e.g. '0,1')")
    parser.add_argument("--hf_token", type=str, default="",
                        help="Hugging Face access token (optional)")

    # ë°ì´í„° ê´€ë ¨
    parser.add_argument("--input_json_file", type=str, required=True,
                        help="Path to input JSON file for evaluation")
    parser.add_argument("--output_json_file", type=str, required=True,
                        help="Path to save evaluation results")
    parser.add_argument("--process_solution_num", type=int, default=None,
                        help="Process only the first N solutions per question")
    parser.add_argument("--include_options", type=str,
                        choices=["yes", "no"], default="yes",
                        help="Include the options in the question text")

    # RAG ì‚¬ìš© ì—¬ë¶€
    parser.add_argument("--use_rag", type=str,
                        choices=["yes", "no"], default="yes",
                        help="'yes': use related_docs / 'no': base PRM only")
    parser.add_argument("--max_token_len", type=int, default=4096,
                        help="Token budget when use_rag is 'yes'")
    parser.add_argument("--use_orm", choices=["yes", "no"], default="no",
                   help="'yes': use orm_processed_solution when RAG is off") 
    parser.add_argument(
        "--data_source_list", type=str, default=None,
        help='JSON-array í˜•ì‹ìœ¼ë¡œ ì¶”ë¡ í•  data_source ì´ë¦„ë“¤ë§Œ ì§€ì • '
            '(ì˜ˆ: \'["medqa","pubmedqa"]\'). ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì „ì²´ ì‚¬ìš©'
    )       

    return parser.parse_args()


# ----------------------------------------------------------------------
# 2. ìœ í‹¸ í•¨ìˆ˜
# ----------------------------------------------------------------------
def format_question_with_options(item):
    """
    "ì§ˆë¬¸ ë³¸ë¬¸ (A) ì˜µì…˜1 (B) ì˜µì…˜2..." í˜•íƒœë¡œ êµ¬ì„±
    """
    q = item.get("question", "")
    opts = item.get("options", [])
    if not opts:
        return q
    return q + "".join(f" ({chr(ord('A') + i)}) {opt}"
                       for i, opt in enumerate(opts))


def truncate_related_docs(docs, tokenizer,
                          max_total_len: int,
                          reserve_for_q_and_sol: int = 1024):
    """
    ê´€ë ¨ ë¬¸ì„œë¥¼ í† í° ìˆ˜ í•œë„ ë‚´ë¡œ ìë¥´ëŠ” í•¨ìˆ˜ (RAG ëª¨ë“œ ì „ìš©)
    """
    kept, used = [], 0
    budget = max_total_len - reserve_for_q_and_sol
    for doc in docs:
        tok_len = len(tokenizer(doc, add_special_tokens=False)["input_ids"])
        if used + tok_len + 1 > budget:
            break
        kept.append(doc)
        used += tok_len + 1
    return kept


# ----------------------------------------------------------------------
# 3. ë©”ì¸ ë¡œì§
# ----------------------------------------------------------------------
def main():
    args = parse_args()
    raw_src_arg = args.data_source_list

    print("====== í‰ê°€ ì„¤ì • ======")
    print(f"ëª¨ë¸ ê²½ë¡œ: {args.model_save_path}")
    print(f"ì…ë ¥ íŒŒì¼: {args.input_json_file}")
    print(f"ì¶œë ¥ íŒŒì¼: {args.output_json_file}")
    print(f"RAG ì‚¬ìš©: {args.use_rag}")
    print(f"ORM ì‚¬ìš©: {args.use_orm}")
    print("=====================")

    if not raw_src_arg:              # None ì´ê±°ë‚˜ "" â†’ ì „ì²´ ì‚¬ìš©
        filter_sources = []
    else:
        try:
            filter_sources = json.loads(raw_src_arg)
            assert isinstance(filter_sources, list)
        except Exception:
            raise ValueError("--data_source_list ëŠ” JSON ë°°ì—´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤ "
                            "(ì˜ˆ: '[\"medqa\",\"mmlu\"]')")

    if args.hf_token:
        login(args.hf_token)

    os.environ["CUDA_VISIBLE_DEVICES"] = args.device

    # ëª¨ë¸Â·í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_save_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model_save_path)
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {type(model).__name__}")

    # '+', '-' í† í° ID
    plus_id = tokenizer(" +", add_special_tokens=False)["input_ids"][0]
    minus_id = tokenizer(" -", add_special_tokens=False)["input_ids"][0]
    print(f"plus_id  : {plus_id} ({tokenizer.convert_ids_to_tokens([plus_id])})")
    print(f"minus_id : {minus_id} ({tokenizer.convert_ids_to_tokens([minus_id])})")

    # --------------------------------------------------------------
    # PRM ì ìˆ˜ ê³„ì‚°
    # --------------------------------------------------------------
    def get_prob(text, special_char=" ĞºĞ¸"):
        encoded = tokenizer(
            text, return_tensors="pt", return_offsets_mapping=True,
            add_special_tokens=True
        )
        input_ids = encoded["input_ids"].to(model.device)
        attention_mask = encoded["attention_mask"].to(model.device)
        offsets = encoded["offset_mapping"][0]
        
        with torch.no_grad():
            logits = model(input_ids, attention_mask=attention_mask).logits[0]

        positions = [i for i, (s, e) in enumerate(offsets)
                     if text[s:e] == special_char]

        plus_probs, min_plus, final_plus = [], None, None
        for pos in positions:
            if pos >= logits.size(0):
                continue
            two = torch.stack([logits[pos][plus_id], logits[pos][minus_id]])
            probs = torch.softmax(two, dim=0)
            plus_probs.append(probs[0])
        if plus_probs:
            min_plus = torch.min(torch.stack(plus_probs)).item()
            final_plus = plus_probs[-1].item()
        
        return {
            "plus_probs": plus_probs,
            "min_plus_prob": min_plus,
            "final_plus_prob": final_plus
        }

    # --------------------------------------------------------------
    # JSON ì²˜ë¦¬
    # --------------------------------------------------------------
    def process_json_with_prm():
        print("ğŸ“‚ JSON íŒŒì¼ ë¡œë“œ ì¤‘...")
        with open(args.input_json_file, encoding="utf-8") as f:
            data = json.load(f)
        
        if filter_sources:        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ë¬´ì‹œ
            data = [d for d in data if d.get("data_source") in filter_sources]
        total = len(data)
        print(f"ğŸ“‹ ì²˜ë¦¬í•  ë°ì´í„° í•­ëª© ìˆ˜: {total}")

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ëª¨ë“œë³„)
        RAG_SYSTEM_PROMPT = (
        "You are an evaluator assessing the logicality and validity of the reasoning in each step of the given explanation. "
        "In order to support the evaluation, the relevant documents, the question, and the explanation are provided sequentially. "
        "If the reasoning contains errors, output - after that step. If the reasoning in a step is logical and valid, output + after that step. "
                )

        PRM_SYSTEM_PROMPT = (
            "You are an evaluator assessing the logicality and validity of the reasoning in each step of the given explanation. "
            "In order to support the evaluation, the question and the explanation are provided. "
            "If the reasoning contains errors, output - after that step. If the reasoning in a step is logical and valid, output + after that step."
                )
        ORM_SYSTEM_PROMPT = (
            "You are an evaluator assessing the overall quality and correctness of the final answer in the given explanation. "
            "In order to support the evaluation, the question and the explanation are provided. "
            "If the final answer is incorrect or not well-supported, output -. If the final answer is correct and well-supported, output +."
                )
        # JSON ì²˜ë¦¬ í•¨ìˆ˜ ì•ˆ, with tqdm â€¦ ë°”ë¡œ ìœ„ìª½
        prm_correct = 0          # PRM ë°©ì‹ ì •ë‹µ ê°œìˆ˜
        mv_correct  = 0          # majority voting ì •ë‹µ ê°œìˆ˜
        
        with tqdm(total=total, desc="Processing Questions", unit="q") as pbar:
            for idx, item in enumerate(data):
                # ì§ˆë¬¸ ë¬¸ìì—´
                q_text = (format_question_with_options(item)
                          if args.include_options == "yes"
                          else item.get("question", ""))

                # ì†”ë£¨ì…˜ ìˆ˜ ì œí•œ
                if args.process_solution_num is not None:
                    item["solutions"] = item["solutions"][:args.process_solution_num]
                sols = item["solutions"]

                # RAG ëª¨ë“œë©´ ë¬¸ì„œ ì „ì²˜ë¦¬
                if args.use_rag == "yes":
                    docs = truncate_related_docs(
                        item.get("related_docs", []),
                        tokenizer,
                        max_total_len=args.max_token_len,
                        reserve_for_q_and_sol=1024
                    )
                    doc_block = "".join(f"Document {i+1}: {d}\n\n"
                                        for i, d in enumerate(docs))
                    system_prompt = RAG_SYSTEM_PROMPT
                    sol_key = "prm_processed_solution"
                else:  # RAG off
                    doc_block = ""
                    if args.use_orm == "yes":
                        system_prompt = ORM_SYSTEM_PROMPT
                        sol_key = "orm_processed_solution"
                    else:
                        system_prompt = PRM_SYSTEM_PROMPT
                        sol_key = "prm_processed_solution"

                # ì†”ë£¨ì…˜ë§ˆë‹¤ PRM ì ìˆ˜ ë¶€ì—¬
                for sol_idx, sol in enumerate(sols):
                    sol_text = sol.get(sol_key, "")
                    user_content = f"{doc_block}Question: {q_text}\n\nExplanation: {sol_text}"

                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user",   "content": user_content}
                    ]
                    raw = tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )

                    res = get_prob(raw, special_char=" ĞºĞ¸")
                    plus_probs = [p.item() for p in res["plus_probs"]]
                    sol["PRM_min_score"] = res["min_plus_prob"] if res["min_plus_prob"] is not None else float("-inf")
                    sol["PRM_score"] = res["final_plus_prob"] if res["final_plus_prob"] is not None else float("-inf")
                    sol["PRM_score_list"] = plus_probs

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # â˜… 2-1. PRM ê¸°ë°˜ ì •ë‹µ ì—¬ë¶€
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                valid = [s for s in sols if s["PRM_min_score"] != float("-inf")]
                prm_pred = max(valid, key=lambda s: s["PRM_min_score"]) if valid else None
                if prm_pred and prm_pred.get("score", 0) == 1:   # score==1 â†’ ì •ë‹µ
                    prm_correct += 1

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # â˜… 2-2. Majority voting ê¸°ë°˜ ì •ë‹µ ì—¬ë¶€
                #     - ë‹µë³€ ë¬¸ìì—´ì´ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ answer ì„ íƒ
                #     - ê·¸ answer ì¤‘ score==1 ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì •ë‹µ ì²˜ë¦¬
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                
                if sols:                                          # ì†”ë£¨ì…˜ì´ ìˆì„ ë•Œë§Œ
                    most_common_ans, _ = Counter(s["answer"] for s in sols).most_common(1)[0]
                    mv_sols = [s for s in sols if s["answer"] == most_common_ans]
                    if any(s.get("score", 0) == 1 for s in mv_sols):
                        mv_correct += 1

                # ê° ë¬¸ì œë§ˆë‹¤ ì •ë‹µë¥  ê³„ì‚° ë° ì¶œë ¥
                current_prm_acc = (prm_correct / (idx + 1)) * 100
                current_mv_acc = (mv_correct / (idx + 1)) * 100
                
                
                # ì§„í–‰ë¥  í‘œì‹œ (ë” ìì„¸í•œ ì •ë³´ í¬í•¨)
                pbar.set_description(f"Q{idx+1}/{total}")
                pbar.set_postfix(
                    PRM=f"{prm_correct}/{idx+1} ({current_prm_acc:.1f}%)",
                    MV=f"{mv_correct}/{idx+1} ({current_mv_acc:.1f}%)"
                )
                pbar.update(1)

        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        with open(args.output_json_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

        print(f"\nâœ… Done. Results saved to {args.output_json_file}")
        print(f"PRM Accuracy : {prm_correct}/{total} ({100*prm_correct/total:.2f}%)")
        print(f"Maj-Vote Acc : {mv_correct}/{total} ({100*mv_correct/total:.2f}%)")
    # ì‹¤í–‰
    process_json_with_prm()

if __name__ == "__main__":
    main()
